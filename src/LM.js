var Matrix = require("ml-matrix");
var math = require("./algebra");

var DEBUG = false;
/** Levenberg Marquardt curve-fitting: minimize sum of weighted squared residuals
 ----------  INPUT  VARIABLES  -----------
 func   = function of n independent variables, 't', and m parameters, 'p',
 returning the simulated model: y_hat = func(t,p,c)
 p      = n-vector of initial guess of parameter values
 t      = m-vectors or matrix of independent variables (used as arg to func)
 y_dat  = m-vectors or matrix of data to be fit by func(t,p)
 weight = weighting vector for least squares fit ( weight >= 0 ) ...
 inverse of the standard measurement errors
 Default:  sqrt(d.o.f. / ( y_dat' * y_dat ))
 dp     = fractional increment of 'p' for numerical derivatives
 dp(j)>0 central differences calculated
 dp(j)<0 one sided 'backwards' differences calculated
 dp(j)=0 sets corresponding partials to zero; i.e. holds p(j) fixed
 Default:  0.001;
 p_min  = n-vector of lower bounds for parameter values
 p_max  = n-vector of upper bounds for parameter values
 c      = an optional matrix of values passed to func(t,p,c)
 opts   = vector of algorithmic parameters
 parameter    defaults    meaning
 opts(1)  =  prnt            3        >1 intermediate results; >2 plots
 opts(2)  =  MaxIter      10*Npar     maximum number of iterations
 opts(3)  =  epsilon_1       1e-3     convergence tolerance for gradient
 opts(4)  =  epsilon_2       1e-3     convergence tolerance for parameters
 opts(5)  =  epsilon_3       1e-3     convergence tolerance for Chi-square
 opts(6)  =  epsilon_4       1e-2     determines acceptance of a L-M step
 opts(7)  =  lambda_0        1e-2     initial value of L-M paramter
 opts(8)  =  lambda_UP_fac   11       factor for increasing lambda
 opts(9)  =  lambda_DN_fac    9       factor for decreasing lambda
 opts(10) =  Update_Type      1       1: Levenberg-Marquardt lambda update
 2: Quadratic update
 3: Nielsen's lambda update equations

 ----------  OUTPUT  VARIABLES  -----------
 p       = least-squares optimal estimate of the parameter values
 X2      = Chi squared criteria
 sigma_p = asymptotic standard error of the parameters
 sigma_y = asymptotic standard error of the curve-fit
 corr    = correlation matrix of the parameters
 R_sq    = R-squared cofficient of multiple determination
 cvg_hst = convergence history

 Henri Gavin, Dept. Civil & Environ. Engineering, Duke Univ. 22 Sep 2013
 modified from: http://octave.sourceforge.net/optim/function/leasqr.html
 using references by
 Press, et al., Numerical Recipes, Cambridge Univ. Press, 1992, Chapter 15.
 Sam Roweis       http://www.cs.toronto.edu/~roweis/notes/lm.pdf
 Manolis Lourakis http://www.ics.forth.gr/~lourakis/levmar/levmar.pdf
 Hans Nielson     http://www2.imm.dtu.dk/~hbn/publ/TR9905.ps
 Mathworks        optimization toolbox reference manual
 K. Madsen, H.B., Nielsen, and O. Tingleff
 http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3215/pdf/imm3215.pdf
 */
var LM = {
  optimize: function (func, p, t, y_dat, weight, dp, p_min, p_max, c, opts) {
    var tensor_parameter = 0; // set to 1 of parameter is a tensor

    var iteration = 0; // iteration counter
    //func_calls = 0;			// running count of function evaluations

    if (typeof p[0] != "object") {
      for (var i = 0; i < p.length; i++) {
        p[i] = [p[i]];
      }
    }
    //p = p(:); y_dat = y_dat(:); 		// make column vectors
    var i, k;
    var eps = 2 ^ -52;
    var Npar = p.length; //length(p); 			// number of parameters
    var Npnt = y_dat.length; //length(y_dat);		// number of data points
    var p_old = Matrix.zeros(Npar, 1); // previous set of parameters
    var y_old = Matrix.zeros(Npnt, 1); // previous model, y_old = y_hat(t;p_old)
    var X2 = 1e-2 / eps; // a really big initial Chi-sq value

    var X2_old = 1e-2 / eps; // a really big initial Chi-sq value
    var J = Matrix.zeros(Npnt, Npar);

    if (t.length != y_dat.length) {
      console.log("lm.m error: the length of t must equal the length of y_dat");

      length_t = t.length;
      length_y_dat = y_dat.length;
      var X2 = 0,
        corr = 0,
        sigma_p = 0,
        sigma_y = 0,
        R_sq = 0,
        cvg_hist = 0;
      if (!tensor_parameter) {
        return;
      }
    }

    weight =
      weight ||
      Math.sqrt(
        (Npnt - Npar + 1) / math.multiply(math.transpose(y_dat), y_dat)
      );
    dp = dp || 0.001;
    p_min = p_min || math.multiply(Math.abs(p), -100);
    p_max = p_max || math.multiply(Math.abs(p), 100);
    c = c || 1;
    // Algorithmic Paramters
    //prnt MaxIter  eps1  eps2  epx3  eps4  lam0  lamUP lamDN UpdateType
    opts = opts || [3, 10 * Npar, 1e-3, 1e-3, 1e-3, 1e-2, 1e-2, 11, 9, 1];

    var prnt = opts[0]; // >1 intermediate results; >2 plots
    var MaxIter = opts[1]; // maximum number of iterations
    var epsilon_1 = opts[2]; // convergence tolerance for gradient
    var epsilon_2 = opts[3]; // convergence tolerance for parameter
    var epsilon_3 = opts[4]; // convergence tolerance for Chi-square
    var epsilon_4 = opts[5]; // determines acceptance of a L-M step
    var lambda_0 = opts[6]; // initial value of damping paramter, lambda
    var lambda_UP_fac = opts[7]; // factor for increasing lambda
    var lambda_DN_fac = opts[8]; // factor for decreasing lambda
    var Update_Type = opts[9]; // 1: Levenberg-Marquardt lambda update
    // 2: Quadratic update
    // 3: Nielsen's lambda update equations

    if (tensor_parameter && prnt == 3) prnt = 2;

    if (!dp.length || dp.length == 1) {
      var dp_array = new Array(Npar);
      for (var i = 0; i < Npar; i++) dp_array[i] = [dp];
      dp = dp_array;
    }

    // indices of the parameters to be fit
    var idx = [];
    for (i = 0; i < dp.length; i++) {
      if (dp[i][0] != 0) {
        idx.push(i);
      }
    }

    var Nfit = idx.length; // number of parameters to fit
    var stop = false; // termination flag

    var weight_sq = null;
    if (!weight.length || weight.length < Npnt) {
      // squared weighting vector
      var tmp = math.multiply(Matrix.ones(Npnt, 1), weight[0]);
      weight_sq = math.dotMultiply(tmp, tmp);
    } else {
      //weight_sq = (weight(:)).^2;
      weight_sq = math.dotMultiply(weight, weight);
    }

    // initialize Jacobian with finite difference calculation
    var result = this.lm_matx(
      func,
      t,
      p_old,
      y_old,
      1,
      J,
      p,
      y_dat,
      weight_sq,
      dp,
      c
    );
    var JtWJ = result.JtWJ,
      JtWdy = result.JtWdy,
      X2 = result.Chi_sq,
      y_hat = result.y_hat,
      J = result.J;

    if (Math.max(Math.abs(JtWdy)) < epsilon_1) {
      console.log(" *** Your Initial Guess is Extremely Close to Optimal ***");
      console.log(" *** epsilon_1 = ", epsilon_1);
      stop = true;
    }

    switch (Update_Type) {
      case 1: // Marquardt: init'l lambda
        lambda = lambda_0;
        break;
      default: // Quadratic and Nielsen
        lambda = lambda_0 * Math.max(math.diag(JtWJ));
        nu = 2;
    }
    X2_old = X2; // previous value of X2
    var h = null;
    while (!stop && iteration <= MaxIter) {
      // --- Main Loop
      iteration = iteration + 1;
      // incremental change in parameters
      switch (Update_Type) {
        case 1: // Marquardt
          // console.log({
          //   A: math.add(
          //     JtWJ,
          //     math.multiply(math.diag(math.diag(JtWJ)), lambda)
          //   ),
          //   B: JtWdy,
          // });
          h = math.solve(
            math.add(JtWJ, math.multiply(math.diag(math.diag(JtWJ)), lambda)),
            JtWdy
          );
          break;
        default: // Quadratic and Nielsen
          h = math.solve(
            math.add(JtWJ, math.multiply(Matrix.eye(Npar), lambda)),
            JtWdy
          );
      }

      var hidx = new Array(idx.length);
      for (k = 0; k < idx.length; k++) {
        hidx[k] = h[idx[k]];
      }
      var p_try = math.add(p, hidx); // update the [idx] elements

      for (k = 0; k < p_try.length; k++) {
        p_try[k][0] = Math.min(Math.max(p_min[k][0], p_try[k][0]), p_max[k][0]);
      }
      // p_try = Math.min(Math.max(p_min,p_try),p_max);           // apply constraints

      var delta_y = math.subtract(y_dat, func(t, p_try, c)); // residual error using p_try
      //func_calls = func_calls + 1;
      //X2_try = delta_y' * ( delta_y .* weight_sq );  // Chi-squared error criteria

      var X2_try = math.multiply(
        math.transpose(delta_y),
        math.dotMultiply(delta_y, weight_sq)
      );

      if (Update_Type == 2) {
        // Quadratic
        //    One step of quadratic line update in the h direction for minimum X2
        //var alpha =  JtWdy'*h / ( (X2_try - X2)/2 + 2*JtWdy'*h ) ;
        var JtWdy_th = math.multiply(math.transpose(JtWdy), h);
        var alpha = math.multiply(
          JtWdy_th,
          math.inv(
            math.add(math.multiply(math.subtract(X2_try - X2), 1 / 2)),
            math.multiply(JtWdy_th, 2)
          )
        ); //JtWdy'*h / ( (X2_try - X2)/2 + 2*JtWdy'*h ) ;

        h = math.multiply(alpha, h);
        for (var k = 0; k < idx.length; k++) {
          hidx[k] = h[idx[k]];
        }

        p_try = math.add(p, hidx); // update only [idx] elements
        p_try = math.min(math.max(p_min, p_try), p_max); // apply constraints

        delta_y = math.subtract(y_dat, func(t, p_try, c)); // residual error using p_try
        // func_calls = func_calls + 1;
        //X2_try = delta_y' * ( delta_y .* weight_sq ); // Chi-squared error criteria
        X2_try = math.multiply(
          math.transpose(delta_y),
          mat.dotMultiply(delta_y, weight_sq)
        );
      }

      //rho = (X2 - X2_try) / ( 2*h' * (lambda * h + JtWdy) ); // Nielsen
      var rho =
        (X2 - X2_try) /
        math.multiply(
          math.multiply(math.transpose(h), 2),
          math.add(math.multiply(lambda, h), JtWdy)
        );
      //console.log("rho "+rho);
      if (rho > epsilon_4) {
        // it IS significantly better
        dX2 = X2 - X2_old;
        X2_old = X2;
        p_old = p;
        y_old = y_hat;
        p = p_try;

        result = this.lm_matx(
          func,
          t,
          p_old,
          y_old,
          dX2,
          J,
          p,
          y_dat,
          weight_sq,
          dp,
          c
        );
        (JtWJ = result.JtWJ),
          (JtWdy = result.JtWdy),
          (X2 = result.Chi_sq),
          (y_hat = result.y_hat),
          (J = result.J);
        // decrease lambda ==> Gauss-Newton method

        switch (Update_Type) {
          case 1: // Levenberg
            lambda = Math.max(lambda / lambda_DN_fac, 1e-7);
            break;
          case 2: // Quadratic
            lambda = Math.max(lambda / (1 + alpha), 1e-7);
            break;
          case 3: // Nielsen
            lambda = math.multiply(
              Math.max(1 / 3, (1 - (2 * rho - 1)) ^ 3),
              lambda
            );
            nu = 2;
            break;
        }
      } else {
        // it IS NOT better
        X2 = X2_old; // do not accept p_try
        if (iteration % (2 * Npar) == 0) {
          // rank-1 update of Jacobian
          result = this.lm_matx(
            func,
            t,
            p_old,
            y_old,
            -1,
            J,
            p,
            y_dat,
            weight_sq,
            dp,
            c
          );
          (JtWJ = result.JtWJ),
            (JtWdy = result.JtWdy),
            (dX2 = result.Chi_sq),
            (y_hat = result.y_hat),
            (J = result.J);
        }

        // increase lambda  ==> gradient descent method
        switch (Update_Type) {
          case 1: // Levenberg
            lambda = Math.min(lambda * lambda_UP_fac, 1e7);
            break;
          case 2: // Quadratic
            lambda = lambda + Math.abs((X2_try - X2) / 2 / alpha);
            break;
          case 3: // Nielsen
            lambda = lambda * nu;
            nu = 2 * nu;
            break;
        }
      }
    } // --- End of Main Loop

    // --- convergence achieved, find covariance and confidence intervals

    // equal weights for parameter error analysis
    weight_sq = math.multiply(
      math.multiply(math.transpose(delta_y), delta_y),
      Matrix.ones(Npnt, 1)
    );

    weight_sq.apply(function (i, j) {
      weight_sq[i][j] = (Npnt - Nfit + 1) / weight_sq[i][j];
    });
    result = this.lm_matx(
      func,
      t,
      p_old,
      y_old,
      -1,
      J,
      p,
      y_dat,
      weight_sq,
      dp,
      c
    );
    (JtWJ = result.JtWJ),
      (JtWdy = result.JtWdy),
      (X2 = result.Chi_sq),
      (y_hat = result.y_hat),
      (J = result.J);

    return { p: p, X2: X2 };
  },

  lm_FD_J: function (func, t, p, y, dp, c) {
    // J = lm_FD_J(func,t,p,y,{dp},{c})
    //
    // partial derivatives (Jacobian) dy/dp for use with lm.m
    // computed via Finite Differences
    // Requires n or 2n function evaluations, n = number of nonzero values of dp
    // -------- INPUT VARIABLES ---------
    // func = function of independent variables, 't', and parameters, 'p',
    //        returning the simulated model: y_hat = func(t,p,c)
    // t  = m-vector of independent variables (used as arg to func)
    // p  = n-vector of current parameter values
    // y  = func(t,p,c) n-vector initialised by user before each call to lm_FD_J
    // dp = fractional increment of p for numerical derivatives
    //      dp(j)>0 central differences calculated
    //      dp(j)<0 one sided differences calculated
    //      dp(j)=0 sets corresponding partials to zero; i.e. holds p(j) fixed
    //      Default:  0.001;
    // c  = optional vector of constants passed to y_hat = func(t,p,c)
    //---------- OUTPUT VARIABLES -------
    // J  = Jacobian Matrix J(i,j)=dy(i)/dp(j)	i=1:n; j=1:m

    //   Henri Gavin, Dept. Civil & Environ. Engineering, Duke Univ. November 2005
    //   modified from: ftp://fly.cnuce.cnr.it/pub/software/octave/leasqr/
    //   Press, et al., Numerical Recipes, Cambridge Univ. Press, 1992, Chapter 15.

    var m = y.length; // number of data points
    var n = p.length; // number of parameters
    var y1;
    dp = dp || math.multiply(Matrix.ones(n, 1), 0.001);

    var ps = p.clone();

    var J = new Matrix(m, n),
      del = new Array(n); // initialize Jacobian to Zero

    for (var j = 0; j < n; j++) {
      del[j] = dp[j] * (1 + Math.abs(p[j][0])); // parameter perturbation
      p[j] = [ps[j][0] + del[j]];

      if (del[j] != 0) {
        y1 = func(t, p, c);
        if (dp[j][0] < 0) {
          // backwards difference
          var column = math.dotDivide(math.subtract(y1, y), del[j]);
          for (var k = 0; k < m; k++) {
            J[k][j] = column[k][0];
          }
          //console.log(column);
        } else {
          p[j][0] = ps[j][0] - del[j];
          var column = math.dotDivide(
            math.subtract(y1, func(t, p, c)),
            2 * del[j]
          );
          for (var k = 0; k < m; k++) {
            J[k][j] = column[k][0];
          }
        } // central difference, additional func call
      }

      p[j] = ps[j]; // restore p(j)
    }
    return J;
  },

  lm_Broyden_J: function (p_old, y_old, J, p, y) {
    // J = lm_Broyden_J(p_old,y_old,J,p,y)
    // carry out a rank-1 update to the Jacobian matrix using Broyden's equation
    //---------- INPUT VARIABLES -------
    // p_old = previous set of parameters
    // y_old = model evaluation at previous set of parameters, y_hat(t;p_old)
    // J  = current version of the Jacobian matrix
    // p     = current  set of parameters
    // y     = model evaluation at current  set of parameters, y_hat(t;p)
    //---------- OUTPUT VARIABLES -------
    // J = rank-1 update to Jacobian Matrix J(i,j)=dy(i)/dp(j)	i=1:n; j=1:m
    var h = math.subtract(p, p_old);

    var h_t = math.transpose(h);
    h_t.div(math.multiply(h_t, h));

    J = math.add(
      J,
      math.multiply(math.subtract(y, math.add(y_old, math.multiply(J, h))), h_t)
    );
    return J;
  },

  lm_matx: function (
    func,
    t,
    p_old,
    y_old,
    dX2,
    J,
    p,
    y_dat,
    weight_sq,
    dp,
    c,
    iteration
  ) {
    // [JtWJ,JtWdy,Chi_sq,y_hat,J] = this.lm_matx(func,t,p_old,y_old,dX2,J,p,y_dat,weight_sq,{da},{c})
    //
    // Evaluate the linearized fitting matrix, JtWJ, and vector JtWdy,
    // and calculate the Chi-squared error function, Chi_sq
    // Used by Levenberg-Marquard algorithm, lm.m
    // -------- INPUT VARIABLES ---------
    // func   = function of n independent variables, t, and m parameters, p,
    //         returning the simulated model: y_hat = func(t,p,c)
    // t      = m-vectors or matrix of independent variables (used as arg to func)
    // p_old  = n-vector of previous parameter values
    // y_old  = m-vector of previous model ... y_old = y_hat(t;p_old);
    // dX2    = previous change in Chi-squared criteria
    // J   = m-by-n Jacobian of model, y_hat, with respect to parameters, p
    // p      = n-vector of current  parameter values
    // y_dat  = n-vector of data to be fit by func(t,p,c)
    // weight_sq = square of the weighting vector for least squares fit ...
    //	    inverse of the standard measurement errors
    // dp     = fractional increment of 'p' for numerical derivatives
    //          dp(j)>0 central differences calculated
    //          dp(j)<0 one sided differences calculated
    //          dp(j)=0 sets corresponding partials to zero; i.e. holds p(j) fixed
    //          Default:  0.001;
    // c      = optional vector of constants passed to y_hat = func(t,p,c)
    //---------- OUTPUT VARIABLES -------
    // JtWJ	 = linearized Hessian matrix (inverse of covariance matrix)
    // JtWdy   = linearized fitting vector
    // Chi_sq = Chi-squared criteria: weighted sum of the squared residuals (WSSR)
    // y_hat  = model evaluated with parameters 'p'
    // J   = m-by-n Jacobian of model, y_hat, with respect to parameters, p

    //   Henri Gavin, Dept. Civil & Environ. Engineering, Duke Univ. November 2005
    //   modified from: ftp://fly.cnuce.cnr.it/pub/software/octave/leasqr/
    //   Press, et al., Numerical Recipes, Cambridge Univ. Press, 1992, Chapter 15.

    var Npnt = y_dat.length;
    var Npar = p.length;

    dp = dp || 0.001;

    var y_hat = func(t, p, c);

    if (iteration % (2 * Npar) == 0 || dX2 > 0) {
      J = this.lm_FD_J(func, t, p, y_hat, dp, c);
    } else {
      J = this.lm_Broyden_J(p_old, y_old, J, p, y_hat); // rank-1 update
    }
    console.log({ J });
    var delta_y = math.subtract(y_dat, y_hat); // residual error between model and data

    var Chi_sq = math.multiply(
      math.transpose(delta_y),
      math.rowMultiply(delta_y, weight_sq)
    );
    var Jt = math.transpose(J);

    var JtWJ = math.multiply(
      Jt,
      math.dotMultiply(J, math.multiply(weight_sq, Matrix.ones(1, Npar)))
    );

    var JtWdy = math.multiply(Jt, math.rowMultiply(delta_y, weight_sq));

    return { JtWJ: JtWJ, JtWdy: JtWdy, Chi_sq: Chi_sq, y_hat: y_hat, J: J };
  },
};

function distance(v1, v2) {
  const nbPoints = v1.length;
  const nbDimensions = v1[0].length;
  let result = new Array(nbPoints);

  for (let point = 0; point < nbPoints; point++) {
    let squaredDistance = 0;
    for (let i; i < nbDimensions; i++) {
      squaredDistance += v1[point][i];
    }
  }
}

module.exports = LM;
